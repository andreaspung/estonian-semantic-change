{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "Parsing and preprocessing the raw corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "import re\n",
    "from estnltk import Text\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def parse_corpus(filename):\n",
    "    \"\"\"\n",
    "    Reads in one corpus file, dealing with input file irregularities.\n",
    "    Returns words and lemmas of the input file as a 2D list of sentences of words. \n",
    "    \"\"\"\n",
    "    all_lemmas = []\n",
    "    all_words = []\n",
    "\n",
    "    with open(f'{DATA_DIR}/{filename}', encoding=\"UTF8\") as f:\n",
    "        for line in f:\n",
    "            if line[:7] in (\"ILU1900\", \"AJA1900\", \"ILU1910\", \"AJA1910\"):\n",
    "                line = line.split(\" \", 1) #split only from the first space\n",
    "            else:\n",
    "                line = line.split(\"    \")\n",
    "            #print(line)\n",
    "            assert len(line) == 2\n",
    "            sentence = html.unescape(line[1]) #&ouml -> ö\n",
    "            \n",
    "            # Writing style normalization\n",
    "            sentence = sentence.replace(\"w\", \"v\") #normalizing the old style of writing - w can be replaced with v\n",
    "            sentence = sentence.replace(\"ß\", \"s\") #normalizing the old style of writing - ß is a German letter\n",
    "            sentence = sentence.replace(\"á\", \"a\")\n",
    "            sentence = sentence.replace(\"ú\", \"u\")\n",
    "            sentence = sentence.replace(\"ñ\", \"n\")\n",
    "            # á, ú, ñ\n",
    "            #estnltk analysis - must not be preprocessed to work as good as possible\n",
    "            text = Text(sentence)\n",
    "            text.tag_layer([\"words\", \"morph_analysis\"])\n",
    "            \n",
    "            #finding tokens and lemmas and filtering out punctuation, numbers\n",
    "            pattern = re.compile(\"[a-zöüõäA-Zöüõä]\") #leave only tokens/lemmas that contain letters and only letters\n",
    "            words = [word.text.lower() for word in text.words if pattern.match(word.text)]\n",
    "            \n",
    "            lemmas = [word[0].lower() for word in text.morph_analysis.lemma if pattern.match(word[0])] #if multiple lemmas available, take the first\n",
    "            \n",
    "            all_words.append(words)\n",
    "            all_lemmas.append(lemmas)\n",
    "            \n",
    "    return all_words, all_lemmas\n",
    "#parse_corpus(\"1900_ilu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding all filenames in the data folder\n",
    "all_files = [f for f in listdir(DATA_DIR) if isfile(join(DATA_DIR, f))]\n",
    "all_files = [filename for filename in all_files if filename[-4:] not in [\".zip\", \"r.gz\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1890_aja',\n",
       " '1890_ilu',\n",
       " '1900_aja',\n",
       " '1900_ilu',\n",
       " '1910_aja',\n",
       " '1910_ilu',\n",
       " '1930_aja',\n",
       " '1930_ilu',\n",
       " '1950_aja',\n",
       " '1950_ilu',\n",
       " '1960_aja',\n",
       " '1960_ilu',\n",
       " '1970_aja',\n",
       " '1970_ilu',\n",
       " '1980_aja',\n",
       " '1980_ilu',\n",
       " '1980_muu',\n",
       " '1980_tea',\n",
       " '1990e_aja',\n",
       " '1990_ilu']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "include_from = 1890 #from which year to include the chosen files\n",
    "files = [filename for filename in all_files if int(filename[:4]) >= include_from]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 20/20 [54:46<00:00, 164.31s/it]\n"
     ]
    }
   ],
   "source": [
    "#Saving the previously found words and lemmas\n",
    "for filename in tqdm(files):\n",
    "    words, lemmas = parse_corpus(filename)\n",
    "    with open(f\"{DATA_DIR}/words/{filename}_words.txt\", mode=\"w+\", encoding=\"utf8\") as f:\n",
    "        for sentence in words:\n",
    "            f.write(\" \".join(sentence))\n",
    "            f.write(\"\\n\")\n",
    "    with open(f\"{DATA_DIR}/lemmas/{filename}_lemmas.txt\", mode=\"w+\", encoding=\"utf8\") as f:\n",
    "        for sentence in lemmas:\n",
    "            f.write(\" \".join(sentence))\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus analytics\n",
    "\n",
    "Finding basic analytics about the corpora.\n",
    "\n",
    "### Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1890\t1900\t1910\t1930\t1950\t1960\t1970\t1980\t1990\t"
     ]
    }
   ],
   "source": [
    "#Words statistical analysis\n",
    "DATA_DIR = \"./corpus/words\" #select either words or lemmas\n",
    "decades = [1890, 1900, 1910, 1930, 1950, 1960, 1970, 1980, 1990] #1920 is a different data domain, 1940 was not included\n",
    "all_files = [f for f in listdir(DATA_DIR) if isfile(join(DATA_DIR, f))]\n",
    "all_files = [filename for filename in all_files if filename[-4:] not in [\".zip\", \"r.gz\"]]\n",
    "\n",
    "for decade in decades:\n",
    "    print(decade, end=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 48835  47743  54398  68032  56561  66196  80872  159813  153227 "
     ]
    }
   ],
   "source": [
    "for decade in decades:\n",
    "    chosen_files = [filename for filename in all_files if int(filename[:4]) == decade]\n",
    "    #print(chosen_files)\n",
    "    \n",
    "    sentences_combined = []\n",
    "    unique_tokens = set()\n",
    "    for filename in chosen_files:\n",
    "        #print(filename)\n",
    "        with open(f\"{DATA_DIR}/{filename}\", mode=\"r\", encoding=\"utf8\") as f:\n",
    "            sentences = []\n",
    "            for row in f:\n",
    "                row = row.strip().split(\" \")\n",
    "                #row = [word.replace(\"w\", \"v\") for word in row] #normalizing the old style of writing\n",
    "                #row = [word.replace(\"ß\", \"s\") for word in row] #normalizing the old style of writing\n",
    "                sentences.append(row)\n",
    "                sentences_combined.append(row)\n",
    "                for token in row:\n",
    "                    unique_tokens.add(token)\n",
    "            # sentences analysis\n",
    "            #print(\"Count of sentences\", len(sentences), end=\" \")\n",
    "            #print(\"Count of words\", sum([len(sentence) for sentence in sentences]), end=\" \")\n",
    "            #print(\"Average sentence length\", round(sum([len(sentence) for sentence in sentences]) / len(sentences)), end=\" \")\n",
    "    #sentences_combined analysis\n",
    "    #print(\"Total count of sentences\", len(sentences_combined), end=\" \")\n",
    "    #print(\"Total count of words\", sum([len(sentence) for sentence in sentences_combined]), end=\" \")\n",
    "    print(\"Total count of unique words\", len(unique_tokens), end=\" \")\n",
    "    #print(\"\", round(sum([len(sentence) for sentence in sentences_combined]) / len(sentences_combined), 2), end=\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmas statistical analysis\n",
    "DATA_DIR = \"./corpus/lemmas\" #select either words or lemmas\n",
    "decades = [1890, 1900, 1910, 1930, 1950, 1960, 1970, 1980, 1990] #1920 is a different data domain, 1940 was not included\n",
    "all_files = [f for f in listdir(DATA_DIR) if isfile(join(DATA_DIR, f))]\n",
    "all_files = [filename for filename in all_files if filename[-4:] not in [\".zip\", \"r.gz\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 26113  24884  28214  33508  27357  33271  41547  76858  74132 "
     ]
    }
   ],
   "source": [
    "for decade in decades:\n",
    "    chosen_files = [filename for filename in all_files if int(filename[:4]) == decade]\n",
    "    #print(chosen_files)\n",
    "    \n",
    "    sentences_combined = []\n",
    "    unique_tokens = set()\n",
    "    for filename in chosen_files:\n",
    "        #print(filename)\n",
    "        with open(DATA_DIR + \"/\" + filename, mode=\"r\", encoding=\"utf8\") as f:\n",
    "            sentences = []\n",
    "            for row in f:\n",
    "                row = row.strip().split(\" \")\n",
    "                row = [word.replace(\"w\", \"v\") for word in row] #normalizing the old style of writing\n",
    "                row = [word.replace(\"ß\", \"s\") for word in row] #normalizing the old style of writing\n",
    "                sentences.append(row)\n",
    "                sentences_combined.append(row)\n",
    "                for token in row:\n",
    "                    unique_tokens.add(token)\n",
    "            # sentences analysis\n",
    "            #print(\"Count of sentences\", len(sentences), end=\" \")\n",
    "            #print(\"Count of lemmas\", sum([len(sentence) for sentence in sentences]), end=\" \")\n",
    "            #print(\"Average sentence length\", round(sum([len(sentence) for sentence in sentences]) / len(sentences)), end=\" \")\n",
    "    #sentences_combined analysis\n",
    "    #print(\"Total count of sentences\", len(sentences_combined), end=\" \")\n",
    "    #print(\"\", sum([len(sentence) for sentence in sentences_combined]), end=\" \")\n",
    "    print(\"\", len(unique_tokens), end=\" \")\n",
    "    #print(\"Average sentence length\", round(sum([len(sentence) for sentence in sentences_combined]) / len(sentences_combined)), end=\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
